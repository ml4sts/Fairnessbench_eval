# 1.  Data representation
fairness_prompt="""
Review the following code and assess whether it includes strategies to identify and fill demographic data gaps. Specifically, check if the code:  
- Assesses where demographic data is missing or underrepresented.
- Does it have a stategy to handle the missing or underrepresented data? 

Provide a score from 0 to 10 based on the following criteria:
- Identification and assessment of demographic data gaps
- Implementation of strategies to fill the gaps

**Code:**
[Insert Code Here]

**1. Evaluation:**
"""
------------

# 2. Data collection and preprocessing
fairness_prompt="""
Review the following code and assess whether privacy-preserving methods have been employed without compromising demographic diversity. Specifically, check if the code:

- Implements privacy-preserving techniques such as differential privacy, data anonymization.
- Ensures that these privacy-preserving methods do not exclude or reduce the representation of any demographic group.
- Strives to maintain a balance between privacy and fairness, ensuring demographic diversity is preserved while addressing privacy concerns.

Provide a score from 0 to 10 based on the following:
- Clear use of privacy-preserving techniques.
- The preservation of demographic diversity when applying privacy methods.
- The balance achieved between privacy and fairness goals in the code.

**Code:**
[Insert Code Here]

**2. Evaluation:**
"""
-------------

# 3. Model training ( Transparency and explainability) 
fairness_prompt="""
Review the following code and assess whether the model's decision-making process is transparent and explainable. Specifically, check for the following:

1. **Interpretable Models**: Does the code use models that are interpretable, such as decision trees, linear models?
2. **Interpretability Techniques**: Are techniques like LIME, SHAP, or other model-agnostic tools used to explain complex models' predictions?
3. **Documentation**: Does the code include clear documentation or comments explaining how the model makes decisions or how certain features contribute to predictions?
4. **Logging/Visualization**: Are there any methods used to visualize or log the model's predictions or feature importances, providing insight into the decision-making process?
5. **Bias Mitigation**: Does the code explain or document efforts to ensure that the model’s decisions do not disproportionately favor or harm certain groups?

Provide a score from 0 to 10 based on the following:
- Clarity of the model's decision-making process.
- Use of interpretable models and/or interpretability techniques.
- Documentation and transparency efforts in explaining the model’s decisions.
- Visualizations or logging to support transparency.

**Code:**
[Insert Code Here]

**3. Evaluation:**
"""
------------

# 4. Evaluation and testing ( Disaggregated Evaluation) 
fairness_prompt="""
Review the following code and assess whether the model’s performance has been evaluated across various demographic groups. Specifically, check for the following:

1. **Disaggregated Performance Metrics**: Does the code evaluate the model's performance for different subgroups (e.g., by race, gender, age, or other demographic factors)?
2. **Fairness Across Groups**: Are there any checks or analysis performed to identify disparities in accuracy or performance across these groups?
3. **Reporting of Group Performance**: Does the code include any steps to report performance metrics (e.g., accuracy, precision, recall) for each demographic group separately?
4. **Addressing Disparities**: If performance disparities are identified, does the code include any steps to address or mitigate these disparities (e.g., reweighting, oversampling underrepresented groups)?
5. **Model Adaptation**: Does the code adapt or modify the model to improve fairness across different demographic groups (e.g., by applying fairness constraints during training)?

Provide a score from 0 to 10 based on the following:
- The presence of disaggregated evaluation for demographic groups.
- The clarity and transparency of performance reporting for these groups.
- Any actions taken to address performance disparities across groups.

**Code:**
[Insert Code Here]

**4. Evaluation:**
"""
----------------

# 5. Evaluation and testing ( Bias Auditing) 
fairness_prompt= """
Review the following code and assess whether regular bias audits are conducted to monitor the model’s fairness over time. Specifically, check for the following:

1. **Regular Bias Audits**: Does the code include steps to regularly evaluate the model for bias? This could include periodic testing on new data or the performance of the model across different demographic groups.
2. **Fairness Metrics Monitoring**: Are fairness metrics (e.g., disparate impact, equal opportunity) used to assess the model’s performance continuously, ensuring it remains fair over time? And these fairness metrics used the right way? 
3. **Model Adjustment for Bias**: If biases are detected during audits, does the code include steps to adjust the model (e.g., retraining with more diverse data, applying fairness constraints, or reweighting the data)?
4. **Adaptation to Evolving Data**: Does the code ensure that the model is adaptable to new data or societal changes, and that any performance disparities are actively addressed?
5. **Logging and Reporting**: Does the code log or report the results of bias audits, providing transparency on the model’s fairness performance and any corrective actions taken?

Provide a score from 0 to 10 based on the following:
- The presence of regular bias audits and their frequency.
- Monitoring and tracking of fairness metrics over time.
- Corrective actions taken when biases are detected.
- Transparency and reporting of the auditing process.

**Code:**
[Insert Code Here]

**5. Evaluation:**

"""